{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image \n",
    "from scipy.ndimage import label\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lightweight U-Net model \n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1): \n",
    "        super(SimpleUNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_ch, out_ch): \n",
    "            return nn.sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1), \n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1), \n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Encoder \n",
    "        self.enc1 = conv_block(in_channels, 32)\n",
    "        self.enc2 = conv_block(32, 64)\n",
    "\n",
    "        # Bottleneck -> Base \n",
    "        self.bottleneck = conv_block(64, 128),\n",
    "\n",
    "        # Decoder (Climbing up)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = conv_block(128, 64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec1 = conv_block(64, 32)\n",
    "\n",
    "        # output \n",
    "        self.final = nn.Conv2d(32, out_channels, 1) # out_channels eq num of classes \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        pool1 = F.max_pool2d(enc1, 2)\n",
    "        enc2 = self.enc2(pool1)\n",
    "        pool2 = F.max_pool2d(enc2, 2)\n",
    "\n",
    "        # Bottleneck BaseLayer \n",
    "        bottleneck = self.bottleneck(pool2)\n",
    "\n",
    "        # Decoder with skip connections \n",
    "        up2 = self.upconv2(bottleneck)\n",
    "        dec2 = self.dec2(torch.cat([up2, enc2], dim=1))\n",
    "\n",
    "        up1 = self.upconv1(dec2)\n",
    "        dec1 = self.dec1(torch.cat([up1, enc1], dim=1))\n",
    "\n",
    "        # output \n",
    "        out = self.final(dec1)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters_deep_learning(image_path, device='cpu'):\n",
    "    # Load and preprocess image \n",
    "    try: \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e: \n",
    "        printf(f\"Error loading image: {e}\")\n",
    "        return \n",
    "\n",
    "    # Resize image to a fixed size for model input \n",
    "    # Data augmentation \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = SimpleUNet().to(device)\n",
    "    model.eval()  \n",
    "\n",
    "    # model.load_state_dict(torch.load('unet_weight-s.pth'))\n",
    "\n",
    "    # Perform inference \n",
    "    with torch.no_grad():\n",
    "        pred_mask = model(img_tensor)\n",
    "\n",
    "    # Convert original image to numpy \n",
    "    img_np = img_tensor.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np * np.array([0.299, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "\n",
    "    # Contour detection on predict mask \n",
    "    labeled, num_features = label(pred_mask)\n",
    "    if num_features == 0:\n",
    "        print(\"No Characters detected\")\n",
    "        return \n",
    "   \n",
    "    # Extract and filter character bounding boxes \n",
    "    characters = []\n",
    "    min_area = 100   # Minimum area\n",
    "    max_aspect = 2.0  # Maximum width / height \n",
    "\n",
    "    for i in range(1, num_features + 1):\n",
    "        contour_mask = (labeled == i).astype(np.uint8)\n",
    "        area = contour_mask.sum()\n",
    "\n",
    "        if area < min_area:\n",
    "            continue \n",
    "\n",
    "        # The goal is to find the coordinate of the characters\n",
    "        y, x = np.where(contour_mask)\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        y_min, y_max = y.min(), y.max()\n",
    "        width = x_max - x_min + 1 \n",
    "        height = y_max - y_min + 1 \n",
    "        aspect_ratio = width / height \n",
    "\n",
    "        if aspect_ration > max_aspect:\n",
    "            num_chars = int(np.ceil(aspect_ratio))\n",
    "            char_width = width // num_chars \n",
    "            for j in range(num_chars):\n",
    "                x_start = x_min + j * char_width \n",
    "                x_end  = min(x_start + char_width, x_max)\n",
    "                characters.append((x_start, x_end, y_min, y_max))\n",
    "        else: \n",
    "            characters.append((x_min, x_max, y_min, y_max)) \n",
    "\n",
    "    # Sort characters by x - coordinate \n",
    "    characters.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Visualize \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(img_np)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(pred_mask, cmap='gray')\n",
    "    plt.title('Predict Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Draw bounding boxes \n",
    "    result_img = img_np.copy()\n",
    "    for i, (x_min, x_max, y_min, y_max) in enumerate(characters):\n",
    "        result_img[y_min:y_max+1, x_min:x_min+2] = [0, 1, 0]\n",
    "        result_img[y_min:y_max+1, x_max-1:x_max+1] = [0, 1, 0]\n",
    "        result_img[y_min:y_max+2, x_min:x_max+1] = [0, 1, 0]\n",
    "        result_img[y_max-1:y_max+1, x_min:x_max+1] = [0, 1, 0]\n",
    "\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(result_img)\n",
    "    plt.title('Segmented Characters')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    # Save individual characters \n",
    "    for i, (x_min, x_max, y_min, y_max) in enumerate(characters):\n",
    "        char_img = img_np[y_min:y_max+1, x_min:x_max+1]\n",
    "        outpath = f'char_{i}'\n",
    "        Image.fromarray((char_img * 255).astype(np.uint8)).save(output_path)\n",
    "        print('Saved character {i} as:', {output_path})\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'sequential'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_text.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43msegment_characters_deep_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36msegment_characters_deep_learning\u001b[1;34m(image_path, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet_weight-s.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mSimpleUNet.__init__\u001b[1;34m(self, in_channels, out_channels)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39msequential(\n\u001b[0;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(in_ch, out_ch, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m      9\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[0;32m     10\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(out_ch, out_ch, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m     11\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Encoder \u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc1 \u001b[38;5;241m=\u001b[39m \u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc2 \u001b[38;5;241m=\u001b[39m conv_block(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Bottleneck -> Base \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mSimpleUNet.__init__.<locals>.conv_block\u001b[1;34m(in_ch, out_ch)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv_block\u001b[39m(in_ch, out_ch): \n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequential\u001b[49m(\n\u001b[0;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(in_ch, out_ch, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m      9\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[0;32m     10\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv2d(out_ch, out_ch, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m     11\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'sequential'"
     ]
    }
   ],
   "source": [
    "dirname = \"datasets\"\n",
    "image_path = os.path.join(dirname, \"sample_text.JPG\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "segment_characters_deep_learning(image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
