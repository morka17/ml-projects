{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1 \n",
    "        return counts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pair = max(stats, key=stats.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list of ints (ids), replace all consective occurences of pait with the new token idx \n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if we are at the very last position AND the pair matches, replace it \n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1 \n",
    "    return newids \n",
    "\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A programmer's introduction to Unicode March 3, 2017 Coding . 22 Comments Unicode!\"\"\"\n",
    "tokens = text.encode('utf-8') # raw bytes \n",
    "tokens = list(map(int, tokens))  # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (65, 32) into a new token 256\n",
      "merging (256, 112) into a new token 257\n",
      "merging (257, 114) into a new token 258\n",
      "merging (258, 111) into a new token 259\n",
      "merging (259, 103) into a new token 260\n",
      "merging (260, 114) into a new token 261\n",
      "merging (261, 97) into a new token 262\n",
      "merging (262, 109) into a new token 263\n",
      "merging (263, 109) into a new token 264\n",
      "merging (264, 101) into a new token 265\n",
      "merging (265, 114) into a new token 266\n",
      "merging (266, 39) into a new token 267\n",
      "merging (267, 115) into a new token 268\n",
      "merging (268, 32) into a new token 269\n",
      "merging (269, 105) into a new token 270\n",
      "merging (270, 110) into a new token 271\n",
      "merging (271, 116) into a new token 272\n",
      "merging (272, 114) into a new token 273\n",
      "merging (273, 111) into a new token 274\n",
      "merging (274, 100) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "#  --- \n",
    "vocab_size = 276 # the desired final vocabulary size \n",
    "num_merges = vocab_size - 256 \n",
    "ids = list(tokens)  \n",
    "\n",
    "\n",
    "merges = {}  # (int, int) -> int \n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i \n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 82\n",
      "ids length 62\n",
      "compression ratio: 1.32X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integets), return Python string \n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    return text \n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(65, 32): 256,\n",
       " (256, 112): 257,\n",
       " (257, 114): 258,\n",
       " (258, 111): 259,\n",
       " (259, 103): 260,\n",
       " (260, 114): 261,\n",
       " (261, 97): 262,\n",
       " (262, 109): 263,\n",
       " (263, 109): 264,\n",
       " (264, 101): 265,\n",
       " (265, 114): 266,\n",
       " (266, 39): 267,\n",
       " (267, 115): 268,\n",
       " (268, 32): 269,\n",
       " (269, 105): 270,\n",
       " (270, 110): 271,\n",
       " (271, 116): 272,\n",
       " (272, 114): 273,\n",
       " (273, 111): 274,\n",
       " (274, 100): 275}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) > 2: \n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "\n",
    "        if pair not in merges:\n",
    "            break #  nothing else can be merged \n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens \n",
    "        \n",
    "\n",
    "\n",
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2  = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treate as specific to any given writing systems\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "\n",
    "print(valtext == valtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "import regex as re \n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+}\"\"\")\n",
    "print(re.findall(gpt2pat, \"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', ' ', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', ' \\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"for  i in range(1, 101):\n",
    "    if i % 3 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else: \n",
    "        print(i)\n",
    "\"\"\"\n",
    "\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "\n",
    "# GPT-2 \n",
    "enc = tiktoken.get_encoding(\"gpt-2\")\n",
    "print(enc.encode(\"    Hello world!!!\"))\n",
    "\n",
    "\n",
    "# GPT-4 (merge spaces)\n",
    "enc = tiktoken.get_encoding(\"cli100k_base\")\n",
    "print(enc.encode(\"   hello world!!!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import Counter, defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPETokenizer: \n",
    "    def __init__(self):\n",
    "        # initial vocabulary with common characters an symbols \n",
    "        self.vocab = {chr(i): i for i in range(32, 127)} # ASCII printable chars \n",
    "        self.merges = {}\n",
    "        self.word_to_tokens = {}\n",
    "\n",
    "    def get_stats(self, word_freq): \n",
    "        \"\"\"\"Count pairs of adjacent symbols in with word freqeuncy dictionary. \"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in word_freq.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1): \n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs \n",
    "\n",
    "    def merge_vocab(self, pair, word_freq):\n",
    "        \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
    "        new_word_freq = {}\n",
    "        pattern = re.escape(' '.join(pair))\n",
    "        replacement = ''.join(pair)\n",
    "\n",
    "        for word in word_freq:\n",
    "            new_word = re.sub(pattern, replacement, word)\n",
    "            new_word_freq[new_word] = word_freq[word]\n",
    "            return new_word_freq\n",
    "    \n",
    "    def train(self, text, num_merges=100):\n",
    "        \"\"\"Train the tokenizer on text with a specified number of merges.\"\"\"\n",
    "        # Preprocess text: split into words and add spaces between characters \n",
    "        words = text.lower().split()\n",
    "        word_freq = Counter(words)\n",
    "\n",
    "        # Convert words to characters sequences with spaces \n",
    "        char_words = {}\n",
    "        for word, freq in word_freq.items():\n",
    "            char_word = ' '.join(list(word)) + ' </w>'\n",
    "            char_words[char_word] = freq \n",
    "\n",
    "            # Perform BPE merges ]\n",
    "            for i in range(num_merges):\n",
    "                pairs = self.get_stats(char_words)\n",
    "                if not pairs:\n",
    "                    break \n",
    "                best_pair = pairs.most_common(1)[0][0]\n",
    "                self.merges[best_pair] = i \n",
    "                char_words = self.merge_vocab(best_pair, char_words)\n",
    "            \n",
    "            # Build final vocabulary\n",
    "            self.vocab.upate({''.join(pair): len(self.vocab) + i for pair, i in self.merges.items()}) \n",
    "            self.word_to_tokens = char_words \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize new text based on trained vocabulary\"\"\"\n",
    "        if not self.merges:\n",
    "            raise ValueError(\"Tokenizer must be trained first!\")\n",
    "    \n",
    "        # Preprocess input text \n",
    "        words = text.lower().split()\n",
    "        tokens = []\n",
    "\n",
    "        for word in words:\n",
    "            # start with characters separated by spaces \n",
    "            current = ' '.join(list(word)) + ' </w>'\n",
    "\n",
    "            # Apply all learned merges \n",
    "            while True:\n",
    "                pairs = [(current.split()[i], current.split()[i +1]) for i in range(len(current.split()) -1 )]\n",
    "                valid_pairs = [p for p in pairs if p in self.merges] \n",
    "                if not valid_pairs:\n",
    "                    break\n",
    "\n",
    "                # Find teh earliest merge \n",
    "                best_pair = min(valid_pairs, key=lambda x: self.merges[x])\n",
    "                pattern = re.escape(' '.join(best_pair))\n",
    "                replacement = ''.join(best_pair)\n",
    "                current = re.sub(pattern, replacement, current)\n",
    "\n",
    "                # convert to token IDs \n",
    "                token_words = current.split()\n",
    "                token_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in token_words if token != '</w>']\n",
    "                tokens.extend(token_ids)\n",
    "\n",
    "            return tokens \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " '\\x05': 5,\n",
       " '\\x06': 6,\n",
       " '\\x07': 7,\n",
       " '\\x08': 8,\n",
       " '\\t': 9,\n",
       " '\\n': 10,\n",
       " '\\x0b': 11,\n",
       " '\\x0c': 12,\n",
       " '\\r': 13,\n",
       " '\\x0e': 14,\n",
       " '\\x0f': 15,\n",
       " '\\x10': 16,\n",
       " '\\x11': 17,\n",
       " '\\x12': 18,\n",
       " '\\x13': 19,\n",
       " '\\x14': 20,\n",
       " '\\x15': 21,\n",
       " '\\x16': 22,\n",
       " '\\x17': 23,\n",
       " '\\x18': 24,\n",
       " '\\x19': 25,\n",
       " '\\x1a': 26,\n",
       " '\\x1b': 27,\n",
       " '\\x1c': 28,\n",
       " '\\x1d': 29,\n",
       " '\\x1e': 30,\n",
       " '\\x1f': 31,\n",
       " ' ': 32,\n",
       " '!': 33,\n",
       " '\"': 34,\n",
       " '#': 35,\n",
       " '$': 36,\n",
       " '%': 37,\n",
       " '&': 38,\n",
       " \"'\": 39,\n",
       " '(': 40,\n",
       " ')': 41,\n",
       " '*': 42,\n",
       " '+': 43,\n",
       " ',': 44,\n",
       " '-': 45,\n",
       " '.': 46,\n",
       " '/': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " '2': 50,\n",
       " '3': 51,\n",
       " '4': 52,\n",
       " '5': 53,\n",
       " '6': 54,\n",
       " '7': 55,\n",
       " '8': 56,\n",
       " '9': 57,\n",
       " ':': 58,\n",
       " ';': 59,\n",
       " '<': 60,\n",
       " '=': 61,\n",
       " '>': 62,\n",
       " '?': 63,\n",
       " '@': 64,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90,\n",
       " '[': 91,\n",
       " '\\\\': 92,\n",
       " ']': 93,\n",
       " '^': 94,\n",
       " '_': 95,\n",
       " '`': 96,\n",
       " 'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " '{': 123,\n",
       " '|': 124,\n",
       " '}': 125,\n",
       " '~': 126,\n",
       " '\\x7f': 127,\n",
       " '\\x80': 128,\n",
       " '\\x81': 129,\n",
       " '\\x82': 130,\n",
       " '\\x83': 131,\n",
       " '\\x84': 132,\n",
       " '\\x85': 133,\n",
       " '\\x86': 134,\n",
       " '\\x87': 135,\n",
       " '\\x88': 136,\n",
       " '\\x89': 137,\n",
       " '\\x8a': 138,\n",
       " '\\x8b': 139,\n",
       " '\\x8c': 140,\n",
       " '\\x8d': 141,\n",
       " '\\x8e': 142,\n",
       " '\\x8f': 143,\n",
       " '\\x90': 144,\n",
       " '\\x91': 145,\n",
       " '\\x92': 146,\n",
       " '\\x93': 147,\n",
       " '\\x94': 148,\n",
       " '\\x95': 149,\n",
       " '\\x96': 150,\n",
       " '\\x97': 151,\n",
       " '\\x98': 152,\n",
       " '\\x99': 153,\n",
       " '\\x9a': 154,\n",
       " '\\x9b': 155,\n",
       " '\\x9c': 156,\n",
       " '\\x9d': 157,\n",
       " '\\x9e': 158,\n",
       " '\\x9f': 159,\n",
       " '\\xa0': 160,\n",
       " 'Â¡': 161,\n",
       " 'Â¢': 162,\n",
       " 'Â£': 163,\n",
       " 'Â¤': 164,\n",
       " 'Â¥': 165,\n",
       " 'Â¦': 166,\n",
       " 'Â§': 167,\n",
       " 'Â¨': 168,\n",
       " 'Â©': 169,\n",
       " 'Âª': 170,\n",
       " 'Â«': 171,\n",
       " 'Â¬': 172,\n",
       " '\\xad': 173,\n",
       " 'Â®': 174,\n",
       " 'Â¯': 175,\n",
       " 'Â°': 176,\n",
       " 'Â±': 177,\n",
       " 'Â²': 178,\n",
       " 'Â³': 179,\n",
       " 'Â´': 180,\n",
       " 'Âµ': 181,\n",
       " 'Â¶': 182,\n",
       " 'Â·': 183,\n",
       " 'Â¸': 184,\n",
       " 'Â¹': 185,\n",
       " 'Âº': 186,\n",
       " 'Â»': 187,\n",
       " 'Â¼': 188,\n",
       " 'Â½': 189,\n",
       " 'Â¾': 190,\n",
       " 'Â¿': 191,\n",
       " 'Ã': 192,\n",
       " 'Ã': 193,\n",
       " 'Ã': 194,\n",
       " 'Ã': 195,\n",
       " 'Ã': 196,\n",
       " 'Ã': 197,\n",
       " 'Ã': 198,\n",
       " 'Ã': 199,\n",
       " 'Ã': 200,\n",
       " 'Ã': 201,\n",
       " 'Ã': 202,\n",
       " 'Ã': 203,\n",
       " 'Ã': 204,\n",
       " 'Ã': 205,\n",
       " 'Ã': 206,\n",
       " 'Ã': 207,\n",
       " 'Ã': 208,\n",
       " 'Ã': 209,\n",
       " 'Ã': 210,\n",
       " 'Ã': 211,\n",
       " 'Ã': 212,\n",
       " 'Ã': 213,\n",
       " 'Ã': 214,\n",
       " 'Ã': 215,\n",
       " 'Ã': 216,\n",
       " 'Ã': 217,\n",
       " 'Ã': 218,\n",
       " 'Ã': 219,\n",
       " 'Ã': 220,\n",
       " 'Ã': 221,\n",
       " 'Ã': 222,\n",
       " 'Ã': 223,\n",
       " 'Ã ': 224,\n",
       " 'Ã¡': 225,\n",
       " 'Ã¢': 226,\n",
       " 'Ã£': 227,\n",
       " 'Ã¤': 228,\n",
       " 'Ã¥': 229,\n",
       " 'Ã¦': 230,\n",
       " 'Ã§': 231,\n",
       " 'Ã¨': 232,\n",
       " 'Ã©': 233,\n",
       " 'Ãª': 234,\n",
       " 'Ã«': 235,\n",
       " 'Ã¬': 236,\n",
       " 'Ã­': 237,\n",
       " 'Ã®': 238,\n",
       " 'Ã¯': 239,\n",
       " 'Ã°': 240,\n",
       " 'Ã±': 241,\n",
       " 'Ã²': 242,\n",
       " 'Ã³': 243,\n",
       " 'Ã´': 244,\n",
       " 'Ãµ': 245,\n",
       " 'Ã¶': 246,\n",
       " 'Ã·': 247,\n",
       " 'Ã¸': 248,\n",
       " 'Ã¹': 249,\n",
       " 'Ãº': 250,\n",
       " 'Ã»': 251,\n",
       " 'Ã¼': 252,\n",
       " 'Ã½': 253,\n",
       " 'Ã¾': 254}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
